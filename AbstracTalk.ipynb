{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "73IO1J1noHnQ",
        "kcAglyJIo2aQ"
      ],
      "authorship_tag": "ABX9TyPoPzYetpaGQPekVM2Twpdh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcolorenzi98/AAI-projects/blob/main/AbstracTalk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Assessment 3 Audio Processing and AI in Production\n",
        "Part 2\n",
        "\n",
        "What to Do: Create a Hugging Face Space and publish the code you generated in the previous notebook.\n",
        "\n",
        "How to Do It: Create a comprehensive package with all required files to publish the app. Use Gradio to design the interface. In the interface, specify the app's name, provide a brief description, and mention that your app only accepts PDFs with abstracts. Include examples of working PDFs in the app. Upload your app to Hugging Face Space and ensure it remains accessible throughout the grading period.\n",
        "\n",
        "What to Deliver: Upload a compressed folder with a .zip or .rar extension. The folder should contain all the files that you uploaded to your Hugging Face Space. Please ADD as first line of the app.py file the address of the Space running the app as a Python Comment (see the example below). The app should keep running in order to be tested at the moment of grading."
      ],
      "metadata": {
        "id": "YM1S96SL4SkT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install and import"
      ],
      "metadata": {
        "id": "LWTO67Y7qlCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio torch transformers"
      ],
      "metadata": {
        "id": "l9ZeaaCN9ACu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "9kMogS2DXp9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "muisdzeB8qYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "46Ipr4zY8P5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PDF Reader"
      ],
      "metadata": {
        "id": "73IO1J1noHnQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries + Code"
      ],
      "metadata": {
        "id": "kcAglyJIo2aQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "!pip install pdfminer.six\n",
        "!pip install pdfplumber\n",
        "!pip install pdf2image\n",
        "!pip install Pillow\n",
        "!pip install pytesseract\n",
        "!apt-get install poppler-utils\n",
        "!apt install tesseract-ocr\n",
        "!apt install libtesseract-dev"
      ],
      "metadata": {
        "id": "yZwfsISGoQQu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eb90c9d-5059-4d8e-f943-7a58a39c46ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (20221105)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (41.0.7)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.10.3)\n",
            "Requirement already satisfied: pdfminer.six==20221105 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20221105)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.24.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (41.0.7)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (2.21)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (1.16.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (9.4.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libtesseract-dev is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To read the PDF\n",
        "import PyPDF2\n",
        "# To analyze the PDF layout and extract text\n",
        "from pdfminer.high_level import extract_pages, extract_text\n",
        "from pdfminer.layout import LTTextContainer, LTChar, LTRect, LTFigure\n",
        "# To extract text from tables in PDF\n",
        "import pdfplumber\n",
        "# To extract the images from the PDFs\n",
        "from PIL import Image\n",
        "from pdf2image import convert_from_path\n",
        "# To perform OCR to extract text from images\n",
        "import pytesseract\n",
        "# To remove the additional created files\n",
        "import os"
      ],
      "metadata": {
        "id": "wOih_JZEohbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to extract text\n",
        "\n",
        "def text_extraction(element):\n",
        "    # Extracting the text from the in-line text element\n",
        "    line_text = element.get_text()\n",
        "\n",
        "    # Find the formats of the text\n",
        "    # Initialize the list with all the formats that appeared in the line of text\n",
        "    line_formats = []\n",
        "    for text_line in element:\n",
        "        if isinstance(text_line, LTTextContainer):\n",
        "            # Iterating through each character in the line of text\n",
        "            for character in text_line:\n",
        "                if isinstance(character, LTChar):\n",
        "                    # Append the font name of the character\n",
        "                    line_formats.append(character.fontname)\n",
        "                    # Append the font size of the character\n",
        "                    line_formats.append(character.size)\n",
        "    # Find the unique font sizes and names in the line\n",
        "    format_per_line = list(set(line_formats))\n",
        "\n",
        "    # Return a tuple with the text in each line along with its format\n",
        "    return (line_text, format_per_line)"
      ],
      "metadata": {
        "id": "2hw8T1aFoi3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to crop the image elements from PDFs\n",
        "def crop_image(element, pageObj):\n",
        "    # Get the coordinates to crop the image from the PDF\n",
        "    [image_left, image_top, image_right, image_bottom] = [element.x0,element.y0,element.x1,element.y1]\n",
        "    # Crop the page using coordinates (left, bottom, right, top)\n",
        "    pageObj.mediabox.lower_left = (image_left, image_bottom)\n",
        "    pageObj.mediabox.upper_right = (image_right, image_top)\n",
        "    # Save the cropped page to a new PDF\n",
        "    cropped_pdf_writer = PyPDF2.PdfWriter()\n",
        "    cropped_pdf_writer.add_page(pageObj)\n",
        "    # Save the cropped PDF to a new file\n",
        "    with open('cropped_image.pdf', 'wb') as cropped_pdf_file:\n",
        "        cropped_pdf_writer.write(cropped_pdf_file)\n",
        "\n",
        "# Create a function to convert the PDF to images\n",
        "def convert_to_images(input_file,):\n",
        "    images = convert_from_path(input_file)\n",
        "    image = images[0]\n",
        "    output_file = \"PDF_image.png\"\n",
        "    image.save(output_file, \"PNG\")\n",
        "\n",
        "# Create a function to read text from images\n",
        "def image_to_text(image_path):\n",
        "    # Read the image\n",
        "    img = Image.open(image_path)\n",
        "    # Extract the text from the image\n",
        "    text = pytesseract.image_to_string(img)\n",
        "    return text"
      ],
      "metadata": {
        "id": "v-1AG3OToldR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting tables from the page\n",
        "\n",
        "def extract_table(pdf_path, page_num, table_num):\n",
        "    # Open the pdf file\n",
        "    pdf = pdfplumber.open(pdf_path)\n",
        "    # Find the examined page\n",
        "    table_page = pdf.pages[page_num]\n",
        "    # Extract the appropriate table\n",
        "    table = table_page.extract_tables()[table_num]\n",
        "    return table\n",
        "\n",
        "# Convert table into the appropriate format\n",
        "def table_converter(table):\n",
        "    table_string = ''\n",
        "    # Iterate through each row of the table\n",
        "    for row_num in range(len(table)):\n",
        "        row = table[row_num]\n",
        "        # Remove the line breaker from the wrapped texts\n",
        "        cleaned_row = [item.replace('\\n', ' ') if item is not None and '\\n' in item else 'None' if item is None else item for item in row]\n",
        "        # Convert the table into a string\n",
        "        table_string+=('|'+'|'.join(cleaned_row)+'|'+'\\n')\n",
        "    # Removing the last line break\n",
        "    table_string = table_string[:-1]\n",
        "    return table_string"
      ],
      "metadata": {
        "id": "uNt5Clv6on-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_pdf(pdf_path):\n",
        "  # create a PDF file object\n",
        "  pdfFileObj = open(pdf_path, 'rb')\n",
        "  # create a PDF reader object\n",
        "  pdfReaded = PyPDF2.PdfReader(pdfFileObj)\n",
        "\n",
        "  # Create the dictionary to extract text from each image\n",
        "  text_per_page = {}\n",
        "  # We extract the pages from the PDF\n",
        "  for pagenum, page in enumerate(extract_pages(pdf_path)):\n",
        "      print(\"Elaborating Page_\" +str(pagenum))\n",
        "      # Initialize the variables needed for the text extraction from the page\n",
        "      pageObj = pdfReaded.pages[pagenum]\n",
        "      page_text = []\n",
        "      line_format = []\n",
        "      text_from_images = []\n",
        "      text_from_tables = []\n",
        "      page_content = []\n",
        "      # Initialize the number of the examined tables\n",
        "      table_num = 0\n",
        "      first_element= True\n",
        "      table_extraction_flag= False\n",
        "      # Open the pdf file\n",
        "      pdf = pdfplumber.open(pdf_path)\n",
        "      # Find the examined page\n",
        "      page_tables = pdf.pages[pagenum]\n",
        "      # Find the number of tables on the page\n",
        "      tables = page_tables.find_tables()\n",
        "\n",
        "\n",
        "      # Find all the elements\n",
        "      page_elements = [(element.y1, element) for element in page._objs]\n",
        "      # Sort all the elements as they appear in the page\n",
        "      page_elements.sort(key=lambda a: a[0], reverse=True)\n",
        "\n",
        "      # Find the elements that composed a page\n",
        "      for i,component in enumerate(page_elements):\n",
        "          # Extract the position of the top side of the element in the PDF\n",
        "          pos= component[0]\n",
        "          # Extract the element of the page layout\n",
        "          element = component[1]\n",
        "\n",
        "          # Check if the element is a text element\n",
        "          if isinstance(element, LTTextContainer):\n",
        "              # Check if the text appeared in a table\n",
        "              if table_extraction_flag == False:\n",
        "                  # Use the function to extract the text and format for each text element\n",
        "                  (line_text, format_per_line) = text_extraction(element)\n",
        "                  # Append the text of each line to the page text\n",
        "                  page_text.append(line_text)\n",
        "                  # Append the format for each line containing text\n",
        "                  line_format.append(format_per_line)\n",
        "                  page_content.append(line_text)\n",
        "              else:\n",
        "                  # Omit the text that appeared in a table\n",
        "                  pass\n",
        "\n",
        "          # Check the elements for images\n",
        "          if isinstance(element, LTFigure):\n",
        "              # Crop the image from the PDF\n",
        "              crop_image(element, pageObj)\n",
        "              # Convert the cropped pdf to an image\n",
        "              convert_to_images('cropped_image.pdf')\n",
        "              # Extract the text from the image\n",
        "              image_text = image_to_text('PDF_image.png')\n",
        "              text_from_images.append(image_text)\n",
        "              page_content.append(image_text)\n",
        "              # Add a placeholder in the text and format lists\n",
        "              page_text.append('image')\n",
        "              line_format.append('image')\n",
        "\n",
        "          # Check the elements for tables\n",
        "          if isinstance(element, LTRect):\n",
        "              # If the first rectangular element\n",
        "              if first_element == True and (table_num+1) <= len(tables):\n",
        "                  # Find the bounding box of the table\n",
        "                  lower_side = page.bbox[3] - tables[table_num].bbox[3]\n",
        "                  upper_side = element.y1\n",
        "                  # Extract the information from the table\n",
        "                  table = extract_table(pdf_path, pagenum, table_num)\n",
        "                  # Convert the table information in structured string format\n",
        "                  table_string = table_converter(table)\n",
        "                  # Append the table string into a list\n",
        "                  text_from_tables.append(table_string)\n",
        "                  page_content.append(table_string)\n",
        "                  # Set the flag as True to avoid the content again\n",
        "                  table_extraction_flag = True\n",
        "                  # Make it another element\n",
        "                  first_element = False\n",
        "                  # Add a placeholder in the text and format lists\n",
        "                  page_text.append('table')\n",
        "                  line_format.append('table')\n",
        "\n",
        "                  # Check if we already extracted the tables from the page\n",
        "                  if element.y0 >= lower_side and element.y1 <= upper_side:\n",
        "                      pass\n",
        "                  elif not isinstance(page_elements[i+1][1], LTRect):\n",
        "                      table_extraction_flag = False\n",
        "                      first_element = True\n",
        "                      table_num+=1\n",
        "\n",
        "\n",
        "      # Create the key of the dictionary\n",
        "      dctkey = 'Page_'+str(pagenum)\n",
        "      # Add the list of list as the value of the page key\n",
        "      text_per_page[dctkey]= [page_text, line_format, text_from_images,text_from_tables, page_content]\n",
        "\n",
        "  # Closing the pdf file object\n",
        "  pdfFileObj.close()\n",
        "\n",
        "  try:\n",
        "\n",
        "    # Deleting the additional files created\n",
        "    os.remove('cropped_image.pdf')\n",
        "    os.remove('PDF_image.png')\n",
        "  finally:\n",
        "    return text_per_page"
      ],
      "metadata": {
        "id": "JIqegvczoqpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Functions"
      ],
      "metadata": {
        "id": "xHib2H05Zi-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Extract abstract"
      ],
      "metadata": {
        "id": "UjLo9YtfZlgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_file(files):\n",
        "    file_paths = [file.name for file in files]\n",
        "    return file_paths"
      ],
      "metadata": {
        "id": "MdzaxUj2Y23H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_abstract(path):\n",
        "\n",
        "  text_per_page = read_pdf(path)\n",
        "\n",
        "  abstract_found = False\n",
        "  abstract_content = \"\"\n",
        "  abstract_lenght = 700\n",
        "  start_collecting = False\n",
        "\n",
        "  for num_page in text_per_page:\n",
        "    page_i = text_per_page[num_page][0]\n",
        "\n",
        "    for index, word in enumerate(page_i):\n",
        "      if (\"abstract\" in word.lower() or \"summary\" in word.lower()):\n",
        "        abstract_found = True\n",
        "        start_collecting = True\n",
        "        continue\n",
        "\n",
        "      if start_collecting:\n",
        "        abstract_content += word + ' '\n",
        "        # Check if the collected content contains \"Introduction\" to stop collecting\n",
        "        if \"introduction\" in word.lower():\n",
        "          break\n",
        "\n",
        "    cleaned_abstract = ' '.join(abstract_content.splitlines()).replace('\\n', ' ').replace('  ', ' ')\n",
        "\n",
        "\n",
        "    if abstract_found:\n",
        "      print(\"Abstract found\")\n",
        "      return cleaned_abstract\n",
        "    else:\n",
        "      print(\"Abstract not found\")\n"
      ],
      "metadata": {
        "id": "XXYJ_7zv3X6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_abstract(path):\n",
        "\n",
        "  abstract_article = extract_abstract(path)\n",
        "\n",
        "  INSTRUCTION = \"summarize: \"\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"haining/scientific_abstract_simplification\")\n",
        "  model = AutoModelForSeq2SeqLM.from_pretrained(\"haining/scientific_abstract_simplification\")\n",
        "  input_text = abstract_article\n",
        "  encoding = tokenizer(INSTRUCTION + input_text,\n",
        "                      max_length=672,\n",
        "                      padding='max_length',\n",
        "                      truncation=True,\n",
        "                      return_tensors='pt')\n",
        "\n",
        "  with torch.no_grad():\n",
        "    decoded_ids = model.generate(input_ids=encoding['input_ids'],\n",
        "                                attention_mask=encoding['attention_mask'],\n",
        "                                max_length=512,\n",
        "                                top_p=.9,\n",
        "                                do_sample=True)\n",
        "\n",
        "  summary=tokenizer.decode(decoded_ids[0], skip_special_tokens=True)\n",
        "\n",
        "  # Extract and print only the first sentence\n",
        "  first_sentence = summary.split('.')[0] + '.'\n",
        "  print(first_sentence)\n",
        "  return first_sentence"
      ],
      "metadata": {
        "id": "i9xfJrAzG8Jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_speech(sentence):\n",
        "\n",
        "  #sentence = summarize_abstract (path)\n",
        "\n",
        "  synthesiser = pipeline(\"text-to-speech\", \"suno/bark-small\")\n",
        "\n",
        "  speech = synthesiser(sentence, forward_params={\"do_sample\": True})\n",
        "\n",
        "  audio_float32 = speech[\"audio\"]\n",
        "  sr = speech[\"sampling_rate\"]\n",
        "\n",
        "  #gr.Audio only accept a tuple(int, np.array(int16))\n",
        "  audio_int16 = (audio_float32 * 32767).astype(np.int16)\n",
        "  audio_reshaped = audio_int16.reshape(audio_int16.shape[1])\n",
        "\n",
        "  return sr, audio_reshaped"
      ],
      "metadata": {
        "id": "YRi77PyaG_1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uploading PDF File"
      ],
      "metadata": {
        "id": "_Tvnsuv5ox_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "mTuETThAotJN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "5c1f0523-2038-4fbc-d0b9-8e82db1a30f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f6094443-736a-48f4-a7cb-33f4eb0e503b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f6094443-736a-48f4-a7cb-33f4eb0e503b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Article 7 Efficient Estimation of Word Representations in Vector Space.pdf to Article 7 Efficient Estimation of Word Representations in Vector Space.pdf\n",
            "Saving Article 9 Transformers in Speech Processing_ Survey.pdf to Article 9 Transformers in Speech Processing_ Survey.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test=text_to_speech(\"/content/Article 11 Hidden Technical Debt in Machine Learning Systems.pdf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WbKOdcVhqml",
        "outputId": "ccccc5a4-2ca2-4e7d-e9a1-51ddb8807266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elaborating Page_0\n",
            "Elaborating Page_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function Wave_write.__del__ at 0x7a6d03e69360>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/wave.py\", line 326, in __del__\n",
            "    self.close()\n",
            "  File \"/usr/lib/python3.10/wave.py\", line 444, in close\n",
            "    self._ensure_header_written(0)\n",
            "  File \"/usr/lib/python3.10/wave.py\", line 467, in _ensure_header_written\n",
            "    self._write_header(datasize)\n",
            "  File \"/usr/lib/python3.10/wave.py\", line 471, in _write_header\n",
            "    self._file.write(b'RIFF')\n",
            "ValueError: write to closed file\n",
            "Exception ignored in: <function Wave_write.__del__ at 0x7a6d03e69360>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/wave.py\", line 326, in __del__\n",
            "    self.close()\n",
            "  File \"/usr/lib/python3.10/wave.py\", line 444, in close\n",
            "    self._ensure_header_written(0)\n",
            "  File \"/usr/lib/python3.10/wave.py\", line 467, in _ensure_header_written\n",
            "    self._write_header(datasize)\n",
            "  File \"/usr/lib/python3.10/wave.py\", line 471, in _write_header\n",
            "    self._file.write(b'RIFF')\n",
            "ValueError: write to closed file\n",
            "Exception ignored in: <function Wave_write.__del__ at 0x7a6d03e69360>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/wave.py\", line 326, in __del__\n",
            "    self.close()\n",
            "  File \"/usr/lib/python3.10/wave.py\", line 444, in close\n",
            "    self._ensure_header_written(0)\n",
            "  File \"/usr/lib/python3.10/wave.py\", line 467, in _ensure_header_written\n",
            "    self._write_header(datasize)\n",
            "  File \"/usr/lib/python3.10/wave.py\", line 471, in _write_header\n",
            "    self._file.write(b'RIFF')\n",
            "ValueError: write to closed file\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elaborating Page_2\n",
            "Elaborating Page_3\n",
            "Elaborating Page_4\n",
            "Elaborating Page_5\n",
            "Elaborating Page_6\n",
            "Elaborating Page_7\n",
            "Elaborating Page_8\n",
            "Abstract found\n",
            "The current paper argues that machine learning can provide tremendous benefits for machine-learning applications: It can predict future events, anticipate their outcomes, and respond to changing conditions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:10000 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gradio interface"
      ],
      "metadata": {
        "id": "y5Nf7_mlztuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interface = gr.Blocks()\n",
        "\n",
        "\n",
        "with interface:\n",
        "  gr.Markdown(\n",
        "    \"\"\"\n",
        "    # AbstracTalk\n",
        "    This app let's you upload an article (you can only upload a PDF with an abstract).\n",
        "    It reads the abstract and does not only summarize it in just one sentence,\n",
        "    but also makes it simpler for anybody to understand. Moreover, it also provides\n",
        "    an additional layer of accessibility through spoken versions of the text.\n",
        "    If you are not satisfied with the given summary you can press again the button and have a new summary.\n",
        "    Have fun and master knowledge with AbstracTalk!\n",
        "    \"\"\")\n",
        "\n",
        "  #the interface architecture goes down here\n",
        "  with gr.Row():\n",
        "    with gr.Column():\n",
        "      uploaded_article = gr.File()\n",
        "      gr.Markdown(\"## PDF Examples\")\n",
        "      gr.Examples(\n",
        "        examples=[[os.path.join(os.path.abspath(\"\"), 'Article 7 Efficient Estimation of Word Representations in Vector Space.pdf')],\n",
        "                  [os.path.join(os.path.abspath(\"\"), \"Article 9 Transformers in Speech Processing_ Survey.pdf\")],\n",
        "                  [os.path.join(os.path.abspath(\"\"), \"Article 11 Hidden Technical Debt in Machine Learning Systems.pdf\")]],\n",
        "        inputs=uploaded_article\n",
        "    )\n",
        "\n",
        "    with gr.Column():\n",
        "      summarized_abstract = gr.Textbox(\"One-sentence Abstract\")\n",
        "      talked_abstract = gr.Audio(type=\"numpy\")\n",
        "      with gr.Row():\n",
        "        summary_button = gr.Button(value=\"Summarize Abstract\", size=\"lg\")\n",
        "        tts_button = gr.Button(value=\"Speak Abstract\", size=\"lg\")\n",
        "\n",
        "\n",
        "  #the functionality goes down here\n",
        "\n",
        "  #first column\n",
        "\n",
        "\n",
        "  #second column\n",
        "  summary_button.click(summarize_abstract, inputs=uploaded_article, outputs=summarized_abstract)\n",
        "  tts_button.click(text_to_speech, inputs=summarized_abstract, outputs=talked_abstract)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    interface.launch(debug=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "Gbo3BX0rzvgQ",
        "outputId": "141ef204-6709-4f7b-f81c-dcaa180913b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://b4c359654ce8695360.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b4c359654ce8695360.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L-wRhGVvEJnH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}